1. **Logic Tensor Networks** by Samy Badreddine, Artur d'Avila Garcez, Luciano Serafini, Michael Spranger. https://arxiv.org/abs/2012.13635

This paper introduces a formalism to combine traditional deep-learning (based on sub-symbolic representations) of data with SymbolicAI (based on high-level representations, such as first-order logic). Logic Tensor Networks (LTN) are a neurosymbolic computational model to train on data with additional symbolic knowledge. It represents data using a formalism called Real Logic, which is a differential generalization of first-order logic where truth values take on any values in [0, 1]. The paper defines and demonstrates the rules of Real Logic, and constructs LTNs as models that can reason within this formalism. This paper does not relate to our current task of interpreting Transformers as translating our Boolean functions to a Real Logic representation will not be compatible with Transformer models as LTNs are not Transformer-based.

2. **Graph Tensor Networks: An Intuitive Framework for Designing Large-Scale Neural Learning Systems on Multiple Domains** by Yao Lei Xu, Kriton Konstantinidis, Danilo P. Mandic. https://arxiv.org/abs/2303.13565.

This paper also introduces an alternate formulation for model architectures, called the Graph Tensor Networks (GTN). This is a generalization encompassing many modern architectures such as ConvNets, Attention Networks, and Recurrent Networks, but extends to other architectures that presumably have better inductive biases for particular domains. This is also not relevant to our work as it offers a more general conception of Model Architecture, rather than of Boolean data. 

3. **Tensor Networks in Machine Learning** by Richik Sengupta, Soumik Adhikary, Ivan Oseledets and Jacob Biamonte. https://arxiv.org/pdf/2207.02851.

This paper explores the techniques of decomposing data, quantum states, or high-dimensional multilinear function through compositions of smaller (ex: lower rank) multi-linear maps, called Tensor Networks, in general. Decomposing a function into Boolean Circuits would be an example of this, but this paradigm is generally employed. There is a duality in that a learned ML representation of Data serves as an effective approprimation to a Tensor Network and a Tensor Network is, in some sense, a trained ML representation. This is fairly interesting from the perspective of Mechanistic Interpretability, as our goal can be restated as finding Tensor Networks for Boolean Functions. However, this paper is a survey of very generalized methods and does not appear to introduce any concrete ideas or insights for our use. As per my understanding, any decomposition of Boolean Functions we have considered so far is a Tensor Network.    